{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title-problem1"
      },
      "source": [
        "# Natural Language Processing 1\n",
        "## Exercise sheet 2: Text classification with Logistic Regression and Naive Bayes\n",
        "\n",
        "### Instructions\n",
        "\n",
        "Fill in the numerical answers in the provided code cells. Submit the completed notebook when done. Reasoning is required only where specified.\n",
        "\n",
        "- Numerical answers should be stored in variables in the provided code cells.\n",
        "- Reasoning and detailed calculations should be added in the designated text cells.\n",
        "- Do **not** modify the variable names or the structure of the notebook.\n",
        "- Save and download your notebook with the following naming convention: `STUDENT_ID.ipynb`, where `STUDENT_ID` is your university-assigned ID.\n",
        "- Upload the notebook to Moodle in the **Exercises** section. Make sure the file is correctly named before submitting.\n",
        "\n"
      ],
      "id": "title-problem1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1: Predicting book categories from titles [MG]\n",
        "\n",
        "**Setup:**  We have a dataset of book titles labeled with either AI or Psychology. We first apply a Naive Bayes classifier, then move on to logistic regression with Bag-of-Words representations.\n"
      ],
      "metadata": {
        "id": "Et146NnbMRcV"
      },
      "id": "Et146NnbMRcV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem1-part1"
      },
      "source": [
        "#### Part 1: Naive Bayes Classifier\n",
        "\n",
        "Fill in the variables in the following code cells with:\n",
        "- The class priors: $P(\\texttt{ai})$ and $P(\\texttt{psychology})$.\n",
        "- The values $P(\\texttt{ai}) \\prod_{w} P(w \\mid \\texttt{ai})$ and $P(\\texttt{psychology}) \\prod_{w} P(w \\mid \\texttt{psychology})$.\n",
        "- The predicted class **$c_{NB}$** based on $\\arg\\max$.\n",
        "\n"
      ],
      "id": "problem1-part1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "code-problem1-NB"
      },
      "outputs": [],
      "source": [
        "# Part 1.1: Compute class priors\n",
        "P_ai = 1/2\n",
        "P_psychology = 1/2\n",
        "\n",
        "# Part 1.2: Compute the values for each class (before applying argmax)\n",
        "value_ai = 1/81\n",
        "value_psychology = 1/50\n",
        "\n",
        "# Part 1.3: Determine the predicted class (argmax decision)\n",
        "# write \"ai\" or \"psychology\"\n",
        "c_NB = \"psychology\""
      ],
      "id": "code-problem1-NB",
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem1-part1-reasoning"
      },
      "source": [
        "##### Reasoning for part 1\n",
        "Part 1.1\n",
        "$$\n",
        "\\ P(c) = \\frac{N_c}{m}\\\n",
        "$$\n",
        "\n",
        "Part 1.2\n",
        "$$\n",
        "\\ P(c) \\prod_{k=1}^n P(w_k \\mid c)\\\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\ P(w_k \\mid c) = \\frac{\\text{count}(w_k, c)}{\\sum_{w \\in V} \\text{count}(w, c)}\n",
        "$$\n",
        "\n",
        "Part 1.3\n",
        "$$\n",
        "\\arg\\max_{c \\in \\mathcal{C}} P(c) \\prod_{i=1}^n P(w_i \\mid c)\\\n",
        "$$\n",
        "\n"
      ],
      "id": "problem1-part1-reasoning"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9oDl2uPNQSfq"
      },
      "id": "9oDl2uPNQSfq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 2: Logistic Regression"
      ],
      "metadata": {
        "id": "ZCdVqMOm8Pfp"
      },
      "id": "ZCdVqMOm8Pfp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem1-part2"
      },
      "source": [
        "##### Part 2.1: Compute the BoW vectors"
      ],
      "id": "problem1-part2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "code-problem1-bow"
      },
      "outputs": [],
      "source": [
        "# Part 2.1: BoW vectors for the 8 training docs.\n",
        "# Represent the vectors as lists.\n",
        "# e.g. [1,1,0,3,1,0,0,3,0,0,0,0,0]\n",
        "\"\"\"\n",
        "deep, learning, artificial, intelligence, the, alingment, problem, reinforcement, to, breath, work, survival, brain\n",
        "\"\"\"\n",
        "bow_doc1 = [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "bow_doc2 = [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "bow_doc3 = [0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
        "bow_doc4 = [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
        "bow_doc5 = [0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0]\n",
        "bow_doc6 = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
        "bow_doc7 = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
        "bow_doc8 = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]"
      ],
      "id": "code-problem1-bow",
      "execution_count": 5
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem1-part2b"
      },
      "source": [
        "##### Part 2.2: Choose a weight vector $\\theta$\n"
      ],
      "id": "problem1-part2b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "code-problem1-weight-vector"
      },
      "outputs": [],
      "source": [
        "# Part 2.2: Your logistic regression weight vector\n",
        "# This must be a list of length = size of your vocabulary.\n",
        "# e.g.: [0, 10, 23, ...]\n",
        "theta_logreg = [0, 0, 1, 1, 0, 1, 1, 1, 0, -1, -1, -1, 1]"
      ],
      "id": "code-problem1-weight-vector",
      "execution_count": 6
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title-problem2"
      },
      "source": [
        "### Exercise 2: Improving a sentiment analysis classifier [MG]\n",
        "\n",
        "**Setup**:\n",
        "We have two labeled documents, each in a 10-word vocabulary. We build a logistic regression classifier to assign **positive** vs. **negative** sentiment.\n",
        "\n",
        "We want:\n",
        "- **BoW vectors** for doc1 (`\"the storyline was intriguing and captivating\"`) and doc2 (`\"dull, horribly dull storyline, not intriguing\"`).\n",
        "- A weight vector that yields about 0.75 probability for doc1 (positive) and 0.25 for doc2 (negative).\n",
        "- Cross-entropy losses for each doc.\n",
        "- One gradient-descent step.\n",
        "- New probabilities after the update.\n"
      ],
      "id": "title-problem2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem2-part1"
      },
      "source": [
        "#### Exercise 2, Part 1: Compute the BoW vectors\n",
        "Store each doc's Bag-of-Words as a list of length 10, corresponding to:\n",
        "```\n",
        "1. the\n",
        "2. storyline\n",
        "3. was\n",
        "4. horribly\n",
        "5. intriguing\n",
        "6. and\n",
        "7. dull\n",
        "8. captivating\n",
        "9. , (comma)\n",
        "10. not\n",
        "```\n"
      ],
      "id": "problem2-part1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "code-problem2-bow"
      },
      "outputs": [],
      "source": [
        "# Part 1: doc1_bow, doc2_bow\n",
        "doc1_bow = [1, 1, 1, 0, 1, 1, 0, 1, 0, 0]\n",
        "doc2_bow = [0, 1, 0, 1, 1, 0, 2, 0, 2, 1]"
      ],
      "id": "code-problem2-bow",
      "execution_count": 7
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem2-part2"
      },
      "source": [
        "#### Exercise 2, Part 2: Propose a weight vector\n",
        "We want logistic regression probabilities:\n",
        "\\(\n",
        "P(y=1|doc1) \\approx 0.75,\\quad P(y=1|doc2) \\approx 0.25.\\)\n",
        "\n",
        "Use nonzero weights **only** for these four words:\n",
        "- word 4 (\"horribly\")\n",
        "- word 5 (\"intriguing\")\n",
        "- word 7 (\"dull\")\n",
        "- word 8 (\"captivating\")\n",
        "\n",
        "Set the other 6 weights to zero. Store your final weight vector  in the code cell below."
      ],
      "id": "problem2-part2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "code-problem2-theta"
      },
      "outputs": [],
      "source": [
        "# list of length 10.\n",
        "import numpy as np\n",
        "theta_ex2 = np.array([0, 0, 0, -1, 0.0986, 0, -0.0986, 1, 0, 0])"
      ],
      "id": "code-problem2-theta",
      "execution_count": 8
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem2-part3"
      },
      "source": [
        "#### Exercise 2, Part 3: Cross-entropy loss\n",
        "For doc1 (label=1) and doc2 (label=0), compute:\n",
        "\n",
        "$\\mathcal{L}_{CE}(\\hat{y},y)=-\\bigl[y\\log\\hat{y}+(1-y)\\log(1-\\hat{y})\\bigr].$\n",
        "\n",
        "Store your two scalar values in the code cell below.\n"
      ],
      "id": "problem2-part3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "code-problem2-CE"
      },
      "outputs": [],
      "source": [
        "# Cross-entropy for doc1 (label=1) and doc2 (label=0)\n",
        "ce_doc1 = 0.2876\n",
        "ce_doc2 = 0.2876"
      ],
      "id": "code-problem2-CE",
      "execution_count": 9
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem2-part4"
      },
      "source": [
        "#### Exercise 2, Part 4: One step of gradient descent\n",
        "Use learning rate $\\alpha=0.1$. Provide:\n",
        "1. The gradient w.r.t. your weight vector $\\theta$.\n",
        "2. The updated $\\theta'$ after the gradient step.\n",
        "3. Briefly explain which entries changed and why.\n"
      ],
      "id": "problem2-part4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "code-problem2-grad",
        "outputId": "642ddc4a-6fa2-4502-e092-fd9400935c41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.25  0.   -0.25  0.25  0.   -0.25  0.5  -0.25  0.5   0.25]\n",
            "[ 0.025   0.      0.025  -1.025   0.0986  0.025  -0.1486  1.025  -0.05\n",
            " -0.025 ]\n",
            "Si el gradiente en negativo, entonces ese par치metro aumenta, por lo que influye positivamente para la sigmoide.\n",
            "Si el gradiente en positivo, entonces ese par치metro disminuye, por lo que influye negativamente para la sigmoide.\n"
          ]
        }
      ],
      "source": [
        "# 4.1 The gradient: list of length 10.\n",
        "# theta_ex2 = np.array([0, 0, 0, -1, 0.0986, 0, -0.0986, 1, 0, 0])\n",
        "grad_theta_ex2 = -0.25 * np.array(doc1_bow) + 0.25 * np.array(doc2_bow)\n",
        "\n",
        "# 4.2 The new theta after applying the gradient update.\n",
        "updated_theta_ex2 = theta_ex2 - 0.1 * grad_theta_ex2\n",
        "\n",
        "print(grad_theta_ex2)\n",
        "print(updated_theta_ex2)\n",
        "print(\"Si el gradiente en negativo, entonces ese par치metro aumenta, por lo que influye positivamente para la sigmoide.\")\n",
        "print(\"Si el gradiente en positivo, entonces ese par치metro disminuye, por lo que influye negativamente para la sigmoide.\")"
      ],
      "id": "code-problem2-grad",
      "execution_count": 10
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem2-part5"
      },
      "source": [
        "#### Exercise 2, Part 5: New probabilities\n",
        "With your updated $\\theta'$, recompute the logistic regression output for doc1 and doc2. Are the probabilities closer to the correct labels (doc1=1, doc2=0)?\n"
      ],
      "id": "problem2-part5"
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + math.exp(-z))"
      ],
      "metadata": {
        "id": "naRlgELrZbyB"
      },
      "id": "naRlgELrZbyB",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "code-problem2-newprobs",
        "outputId": "e7eaa6de-ee83-4508-daaa-8439a924207f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7682756376602405\n",
            "0.7939006510813742\n"
          ]
        }
      ],
      "source": [
        "# Recompute doc1 & doc2 probabilities with the updated theta.\n",
        "\n",
        "p_doc1_new = sigmoid(updated_theta_ex2 @ doc1_bow)  # P(y = 1 | x(1))\n",
        "p_doc2_new = 1 - sigmoid(updated_theta_ex2 @ doc2_bow)  #  P(y = 0 | x(2))\n",
        "\n",
        "print(p_doc1_new)\n",
        "print(p_doc2_new)"
      ],
      "id": "code-problem2-newprobs",
      "execution_count": 12
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem2-reasoning"
      },
      "source": [
        "#### Reasoning for Exercise 2\n",
        "Use this cell to provide your step-by-step calculations:\n",
        "\n",
        "- How you chose the weights $\\theta$.\n",
        "Doing a 2 equations system\n",
        "\n",
        "- How you compute the loss.\n",
        "Substituing the training label and the predicted label in the loss formula.\n",
        "\n",
        "- The gradient derivation and weight updates. Do the weight changes make sense to you? Which weights changed, and how do these changes  help the classifier improve its predictions?\n",
        "Yes, the weight updates make sense, the classifier improves the predictions. The words that only appear in doc1 have a negative gradient while doc2 words have a positive gradient. If a word appears in both, they cancel to 0, meaning no gradient.\n",
        "\n",
        "- the final updated probabilities. Did the classification predictions improve? That is, are the predicted probabilities for the true labels now closer to $1$, for all examples?\n",
        "Yes the classification improves. Before they where ~0.75, now ~0.77 ans ~0.79.\n"
      ],
      "id": "problem2-reasoning"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "v9mMABqaQ6D8"
      },
      "id": "v9mMABqaQ6D8"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}